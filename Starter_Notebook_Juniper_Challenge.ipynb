{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kadirdundar/Challenge_of_Juniper_notebooks/blob/main/Starter_Notebook_Juniper_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective:\n",
        "A Retail Superstore chain wants to gain insights on the performance of each its stores and make strategic decisions on its operating model.\n",
        "\n",
        "As a Data Scientist you are required to understand the data provided and help the client predict the sales at the register in current hour based on the sales attribute associated with each store.\n",
        "\n",
        "The given data consists of 4 broad attribute types:\n",
        "   * Register: Attributes related to the register at which the sales transaction has taken place\n",
        "   * Cashier: Attributes related to cashier working at the register\n",
        "   * Store: Attributes related to a particular store\n",
        "   * Region: Attributes related to a group of stores in the region\n",
        "\n",
        "\n",
        "\n",
        "## Problem Statement:\n",
        "Predict the sales at the register in current hour and prioritize the individual attributes that affect the sales.\n"
      ],
      "metadata": {
        "id": "LV_Zm6LD0MkP"
      },
      "id": "LV_Zm6LD0MkP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upload your datasets to colab using upload to storage session taskbar"
      ],
      "metadata": {
        "id": "5z5J4HoOz60m"
      },
      "id": "5z5J4HoOz60m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries"
      ],
      "metadata": {
        "id": "7dQ_ujSBzh0H"
      },
      "id": "7dQ_ujSBzh0H"
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "44e01b6d",
      "metadata": {
        "id": "44e01b6d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib import colors\n",
        "#\n",
        "import xgboost as xgb\n",
        "#\n",
        "import os\n",
        "import sys\n",
        "#\n",
        "from datetime import datetime, timedelta\n",
        "from time import time\n",
        "from uuid import uuid4\n",
        "from scipy.ndimage import convolve1d\n",
        "#\n",
        "from sklearn.metrics import make_scorer, r2_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "# \n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a6ce5de",
      "metadata": {
        "id": "2a6ce5de"
      },
      "source": [
        "## Label Encoding and Null Value Imputaion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "ffcf7816",
      "metadata": {
        "id": "ffcf7816"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9RZyduRwIGg",
        "outputId": "bc354ced-aa80-44ab-c7a8-92a55ed22a33"
      },
      "id": "T9RZyduRwIGg",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/yarisma/training_dataset.csv\")\n"
      ],
      "metadata": {
        "id": "zzl_H_l60UbO"
      },
      "id": "zzl_H_l60UbO",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkDf(dataframe, head = 8):\n",
        "  print(\"##### Shape #####\")\n",
        "  print(dataframe.shape)\n",
        "\n",
        "  print(\"\\n##### Null Analysis #####\")\n",
        "  print(dataframe.isnull().sum())\n",
        "\n",
        "\n",
        "\n",
        "checkDf(df)\n",
        "df.head()  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "AtNk82mk0deR",
        "outputId": "8628ba65-05ca-420e-c3a9-8af95749c1e2"
      },
      "id": "AtNk82mk0deR",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Shape #####\n",
            "(2058, 63)\n",
            "\n",
            "##### Null Analysis #####\n",
            "Unnamed: 0                                       0\n",
            "observation_id                                   0\n",
            "observation_timestamp                            0\n",
            "hour_of_day                                      0\n",
            "register__sales_dollar_amt_this_hour             0\n",
            "                                              ... \n",
            "region__nighttime_returns_amt_per_hour           1\n",
            "region__peak_sales_dollar_amt_per_hour           1\n",
            "region__peak_sales_dollar_amt_per_hour_v2       17\n",
            "region__peak_returns_dollar_amt_per_hour         1\n",
            "region__peak_returns_dollar_amt_per_hour_v2    221\n",
            "Length: 63, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                        observation_id observation_timestamp  \\\n",
              "0           0  704d2a80-d52e-11ec-90ff-c7e6292284b3   2022-05-16 15:39:57   \n",
              "1           1  1cacc1d0-e6ac-11ec-b65d-156af70ce36b   2022-06-07 21:52:23   \n",
              "2           2  6dc2b330-d37a-11ec-884e-dfe9ea4a7bd5   2022-05-14 11:38:52   \n",
              "3           3  163ee0a0-0cca-11ed-a73c-8904b24187cc   2022-07-26 10:02:41   \n",
              "4           4  5e3c5df0-d5ee-11ec-a5f2-3b6f99e95850   2022-05-17 14:33:50   \n",
              "\n",
              "   hour_of_day  register__sales_dollar_amt_this_hour  \\\n",
              "0           15                                347.29   \n",
              "1           21                                361.59   \n",
              "2           11                                850.73   \n",
              "3           10                               1175.69   \n",
              "4           14                               3204.53   \n",
              "\n",
              "  register__payment_types_accepted  register__peak_sales_dollar_amt_per_hour  \\\n",
              "0                      Cash+Credit                                   -0.7383   \n",
              "1                      Cash+Credit                                    0.6483   \n",
              "2                      Cash+Credit                                   -0.4950   \n",
              "3                      Cash+Credit                                   -0.5594   \n",
              "4                      Cash+Credit                                    0.5693   \n",
              "\n",
              "   register__sales_dollar_amt_last_hour  register__sales_quantity_last_hour  \\\n",
              "0                               -0.1270                             -0.1993   \n",
              "1                               -0.0362                             -0.0777   \n",
              "2                               -0.1268                             -0.1974   \n",
              "3                               -0.1270                             -0.1991   \n",
              "4                               -0.1221                             -0.1632   \n",
              "\n",
              "   register__sales_quantity_rescanned_frac  ...  \\\n",
              "0                                  -0.8299  ...   \n",
              "1                                  -0.7395  ...   \n",
              "2                                   1.3139  ...   \n",
              "3                                  -0.8299  ...   \n",
              "4                                  -0.7071  ...   \n",
              "\n",
              "   region__sales_dollar_amt_last_hour  region__returns_dollar_amt_last_hour  \\\n",
              "0                             -0.6920                               -0.4605   \n",
              "1                             -0.6531                               -0.4434   \n",
              "2                             -0.6120                               -0.1786   \n",
              "3                             -0.5472                                0.1246   \n",
              "4                             -0.3838                                0.1996   \n",
              "\n",
              "   region__nighttime_open_registers  \\\n",
              "0                           -0.5180   \n",
              "1                           -0.6498   \n",
              "2                           -0.6040   \n",
              "3                           -0.5925   \n",
              "4                           -0.5696   \n",
              "\n",
              "   region__nighttime_service_time_per_customer  \\\n",
              "0                                      -1.0062   \n",
              "1                                       0.9031   \n",
              "2                                      -1.4229   \n",
              "3                                      -1.4097   \n",
              "4                                       1.1231   \n",
              "\n",
              "   region__nighttime_sales_amt_per_hour  \\\n",
              "0                               -0.6462   \n",
              "1                               -0.6493   \n",
              "2                               -0.6456   \n",
              "3                               -0.6478   \n",
              "4                               -0.6475   \n",
              "\n",
              "   region__nighttime_returns_amt_per_hour  \\\n",
              "0                                 -0.6030   \n",
              "1                                 -0.6106   \n",
              "2                                 -0.6037   \n",
              "3                                 -0.6079   \n",
              "4                                 -0.6032   \n",
              "\n",
              "   region__peak_sales_dollar_amt_per_hour  \\\n",
              "0                                 -0.4773   \n",
              "1                                  0.4998   \n",
              "2                                  1.9337   \n",
              "3                                  0.1288   \n",
              "4                                  1.0050   \n",
              "\n",
              "   region__peak_sales_dollar_amt_per_hour_v2  \\\n",
              "0                                     0.1748   \n",
              "1                                    -0.9816   \n",
              "2                                    -0.9093   \n",
              "3                                    -0.9093   \n",
              "4                                     0.1748   \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour  \\\n",
              "0                                   -1.7951   \n",
              "1                                    0.8939   \n",
              "2                                    2.4046   \n",
              "3                                   -0.4983   \n",
              "4                                    1.5660   \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2  \n",
              "0                                      -0.8284  \n",
              "1                                      -0.8614  \n",
              "2                                      -0.7567  \n",
              "3                                      -0.7567  \n",
              "4                                       0.2356  \n",
              "\n",
              "[5 rows x 63 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f409894-975a-4933-91c6-6d368b4441ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>observation_id</th>\n",
              "      <th>observation_timestamp</th>\n",
              "      <th>hour_of_day</th>\n",
              "      <th>register__sales_dollar_amt_this_hour</th>\n",
              "      <th>register__payment_types_accepted</th>\n",
              "      <th>register__peak_sales_dollar_amt_per_hour</th>\n",
              "      <th>register__sales_dollar_amt_last_hour</th>\n",
              "      <th>register__sales_quantity_last_hour</th>\n",
              "      <th>register__sales_quantity_rescanned_frac</th>\n",
              "      <th>...</th>\n",
              "      <th>region__sales_dollar_amt_last_hour</th>\n",
              "      <th>region__returns_dollar_amt_last_hour</th>\n",
              "      <th>region__nighttime_open_registers</th>\n",
              "      <th>region__nighttime_service_time_per_customer</th>\n",
              "      <th>region__nighttime_sales_amt_per_hour</th>\n",
              "      <th>region__nighttime_returns_amt_per_hour</th>\n",
              "      <th>region__peak_sales_dollar_amt_per_hour</th>\n",
              "      <th>region__peak_sales_dollar_amt_per_hour_v2</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>704d2a80-d52e-11ec-90ff-c7e6292284b3</td>\n",
              "      <td>2022-05-16 15:39:57</td>\n",
              "      <td>15</td>\n",
              "      <td>347.29</td>\n",
              "      <td>Cash+Credit</td>\n",
              "      <td>-0.7383</td>\n",
              "      <td>-0.1270</td>\n",
              "      <td>-0.1993</td>\n",
              "      <td>-0.8299</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.6920</td>\n",
              "      <td>-0.4605</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>-1.0062</td>\n",
              "      <td>-0.6462</td>\n",
              "      <td>-0.6030</td>\n",
              "      <td>-0.4773</td>\n",
              "      <td>0.1748</td>\n",
              "      <td>-1.7951</td>\n",
              "      <td>-0.8284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1cacc1d0-e6ac-11ec-b65d-156af70ce36b</td>\n",
              "      <td>2022-06-07 21:52:23</td>\n",
              "      <td>21</td>\n",
              "      <td>361.59</td>\n",
              "      <td>Cash+Credit</td>\n",
              "      <td>0.6483</td>\n",
              "      <td>-0.0362</td>\n",
              "      <td>-0.0777</td>\n",
              "      <td>-0.7395</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.6531</td>\n",
              "      <td>-0.4434</td>\n",
              "      <td>-0.6498</td>\n",
              "      <td>0.9031</td>\n",
              "      <td>-0.6493</td>\n",
              "      <td>-0.6106</td>\n",
              "      <td>0.4998</td>\n",
              "      <td>-0.9816</td>\n",
              "      <td>0.8939</td>\n",
              "      <td>-0.8614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6dc2b330-d37a-11ec-884e-dfe9ea4a7bd5</td>\n",
              "      <td>2022-05-14 11:38:52</td>\n",
              "      <td>11</td>\n",
              "      <td>850.73</td>\n",
              "      <td>Cash+Credit</td>\n",
              "      <td>-0.4950</td>\n",
              "      <td>-0.1268</td>\n",
              "      <td>-0.1974</td>\n",
              "      <td>1.3139</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.6120</td>\n",
              "      <td>-0.1786</td>\n",
              "      <td>-0.6040</td>\n",
              "      <td>-1.4229</td>\n",
              "      <td>-0.6456</td>\n",
              "      <td>-0.6037</td>\n",
              "      <td>1.9337</td>\n",
              "      <td>-0.9093</td>\n",
              "      <td>2.4046</td>\n",
              "      <td>-0.7567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>163ee0a0-0cca-11ed-a73c-8904b24187cc</td>\n",
              "      <td>2022-07-26 10:02:41</td>\n",
              "      <td>10</td>\n",
              "      <td>1175.69</td>\n",
              "      <td>Cash+Credit</td>\n",
              "      <td>-0.5594</td>\n",
              "      <td>-0.1270</td>\n",
              "      <td>-0.1991</td>\n",
              "      <td>-0.8299</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5472</td>\n",
              "      <td>0.1246</td>\n",
              "      <td>-0.5925</td>\n",
              "      <td>-1.4097</td>\n",
              "      <td>-0.6478</td>\n",
              "      <td>-0.6079</td>\n",
              "      <td>0.1288</td>\n",
              "      <td>-0.9093</td>\n",
              "      <td>-0.4983</td>\n",
              "      <td>-0.7567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5e3c5df0-d5ee-11ec-a5f2-3b6f99e95850</td>\n",
              "      <td>2022-05-17 14:33:50</td>\n",
              "      <td>14</td>\n",
              "      <td>3204.53</td>\n",
              "      <td>Cash+Credit</td>\n",
              "      <td>0.5693</td>\n",
              "      <td>-0.1221</td>\n",
              "      <td>-0.1632</td>\n",
              "      <td>-0.7071</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.3838</td>\n",
              "      <td>0.1996</td>\n",
              "      <td>-0.5696</td>\n",
              "      <td>1.1231</td>\n",
              "      <td>-0.6475</td>\n",
              "      <td>-0.6032</td>\n",
              "      <td>1.0050</td>\n",
              "      <td>0.1748</td>\n",
              "      <td>1.5660</td>\n",
              "      <td>0.2356</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 63 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f409894-975a-4933-91c6-6d368b4441ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f409894-975a-4933-91c6-6d368b4441ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f409894-975a-4933-91c6-6d368b4441ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grabColNames(dataframe, catTh=10, carTh=20):\n",
        "    \"\"\"\n",
        "\n",
        "    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.\n",
        "    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.\n",
        "\n",
        "    Parameters\n",
        "    ------\n",
        "        dataframe: dataframe\n",
        "                Değişken isimleri alınmak istenilen dataframe\n",
        "        catTh: int, optional\n",
        "                numerik fakat kategorik olan değişkenler için sınıf eşik değeri\n",
        "        carTh: int, optinal\n",
        "                kategorik fakat kardinal değişkenler için sınıf eşik değeri\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "        catCols: list\n",
        "                Kategorik değişken listesi\n",
        "        numCols: list\n",
        "                Numerik değişken listesi\n",
        "        catButCar: list\n",
        "                Kategorik görünümlü kardinal değişken listesi\n",
        "\n",
        "    Examples\n",
        "    ------\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"iris\")\n",
        "        print(grabColNames(df))\n",
        "\n",
        "\n",
        "    Notes\n",
        "    ------\n",
        "        catCols + numCols + catButCar = toplam değişken sayısı\n",
        "        numButCat catCols'un içerisinde.\n",
        "        Return olan 3 liste toplamı toplam değişken sayısına eşittir: catCols + numCols + catButCar = değişken sayısı\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # catCols, catButCar\n",
        "    catCols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
        "    numButCat = [col for col in dataframe.columns if dataframe[col].nunique() < catTh and\n",
        "                 dataframe[col].dtypes != \"O\"]\n",
        "    catButCar = [col for col in dataframe.columns if dataframe[col].nunique() > carTh and\n",
        "                 dataframe[col].dtypes == \"O\"]\n",
        "    catCols = catCols + numButCat\n",
        "    catCols = [col for col in catCols if col not in catButCar]\n",
        "\n",
        "    # numCols\n",
        "    numCols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
        "    numCols = [col for col in numCols if col not in numButCat]\n",
        "\n",
        "    print(f\"Observations: {dataframe.shape[0]}\")\n",
        "    print(f\"Variables: {dataframe.shape[1]}\")\n",
        "    print(f'catCols: {len(catCols)}')\n",
        "    print(f'numCols: {len(numCols)}')\n",
        "    print(f'catButCar: {len(catButCar)}')\n",
        "    print(f'numButCat: {len(numButCat)}')\n",
        "    return catCols, numCols, catButCar\n",
        "\n",
        "\n",
        "catCols, numCols, catButCar = grabColNames(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SFJp5c45Z6N",
        "outputId": "87e207b2-4936-4cad-8e01-c958fddab8ec"
      },
      "id": "0SFJp5c45Z6N",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observations: 2058\n",
            "Variables: 63\n",
            "catCols: 10\n",
            "numCols: 51\n",
            "catButCar: 2\n",
            "numButCat: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "catCols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOGRXuJ35hsY",
        "outputId": "c40da929-7b1f-4efe-e3eb-59a9eafc72d6"
      },
      "id": "jOGRXuJ35hsY",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['register__payment_types_accepted',\n",
              " 'store__type_code',\n",
              " 'cashier__title_level',\n",
              " 'cashier__n_years_experience',\n",
              " 'store__n_employees_total',\n",
              " 'store__n_managers',\n",
              " 'store__n_baggers',\n",
              " 'store__is_sufficiently_staffed',\n",
              " 'region__peak_sales_dollar_amt_per_hour_v2',\n",
              " 'region__peak_returns_dollar_amt_per_hour_v2']"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numCols"
      ],
      "metadata": {
        "id": "k_8k9Sf65omh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbae052-fd40-4ba8-f05a-2a545c4296f7"
      },
      "id": "k_8k9Sf65omh",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Unnamed: 0',\n",
              " 'hour_of_day',\n",
              " 'register__sales_dollar_amt_this_hour',\n",
              " 'register__peak_sales_dollar_amt_per_hour',\n",
              " 'register__sales_dollar_amt_last_hour',\n",
              " 'register__sales_quantity_last_hour',\n",
              " 'register__sales_quantity_rescanned_frac',\n",
              " 'register__sales_payments_declined_frac',\n",
              " 'register__peak_returns_dollar_amt_per_hour',\n",
              " 'register__returns_dollar_amt_last_hour',\n",
              " 'register__returns_quantity_last_hour',\n",
              " 'register__returns_quantity_rescanned_frac',\n",
              " 'cashier__hours_into_shift',\n",
              " 'cashier__item_scan_rate_per_min',\n",
              " 'cashier__item_manual_entry_rate_per_min',\n",
              " 'store__miles_to_nearest_location',\n",
              " 'store__target_sales_quantity_per_hour',\n",
              " 'store__mean_customer_to_staff_ratio',\n",
              " 'store__mean_service_time_per_customer',\n",
              " 'store__n_open_registers',\n",
              " 'store__occupancy_main_floor',\n",
              " 'store__occupancy_grocery',\n",
              " 'store__occupancy_checkout_areas',\n",
              " 'store__occupancy_food_court',\n",
              " 'store__occupancy_backrooms',\n",
              " 'store__occupancy_indoors',\n",
              " 'store__occupancy_outdoors',\n",
              " 'store__outdoor_temperature',\n",
              " 'store__parking_lot_utilization',\n",
              " 'store__shelf_freespace_frac',\n",
              " 'store__hrs_since_last_delivery',\n",
              " 'store__sales_dollar_amt_last_hour',\n",
              " 'store__sales_quantity_last_hour',\n",
              " 'store__sales_quantity_rescanned_frac',\n",
              " 'store__gift_sales_quantity_last_hour',\n",
              " 'store__returns_dollar_amt_last_hour',\n",
              " 'store__returns_quantity_last_hour',\n",
              " 'store__returns_quantity_rescanned_frac',\n",
              " 'store__gift_returns_quantity_last_hour',\n",
              " 'region__n_stores',\n",
              " 'region__n_open_registers',\n",
              " 'region__mean_service_time_per_customer',\n",
              " 'region__stdev_service_time_per_customer',\n",
              " 'region__sales_dollar_amt_last_hour',\n",
              " 'region__returns_dollar_amt_last_hour',\n",
              " 'region__nighttime_open_registers',\n",
              " 'region__nighttime_service_time_per_customer',\n",
              " 'region__nighttime_sales_amt_per_hour',\n",
              " 'region__nighttime_returns_amt_per_hour',\n",
              " 'region__peak_sales_dollar_amt_per_hour',\n",
              " 'region__peak_returns_dollar_amt_per_hour']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "catButCar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2ice1yO5uxV",
        "outputId": "bda066d8-ae59-4d57-b74a-f7ac033eff8b"
      },
      "id": "A2ice1yO5uxV",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['observation_id', 'observation_timestamp']"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lI6Odpnw5yw7"
      },
      "id": "lI6Odpnw5yw7",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdfcKQTZ6h8A"
      },
      "id": "ZdfcKQTZ6h8A",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naCols = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
        "print(naCols)"
      ],
      "metadata": {
        "id": "htfLo5kn_AFD",
        "outputId": "5bb685d2-7b69-4331-f8bb-45226559833f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "htfLo5kn_AFD",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['store__gift_returns_quantity_last_hour', 'region__n_stores', 'region__n_open_registers', 'region__mean_service_time_per_customer', 'region__stdev_service_time_per_customer', 'region__sales_dollar_amt_last_hour', 'region__returns_dollar_amt_last_hour', 'region__nighttime_open_registers', 'region__nighttime_service_time_per_customer', 'region__nighttime_sales_amt_per_hour', 'region__nighttime_returns_amt_per_hour', 'region__peak_sales_dollar_amt_per_hour', 'region__peak_sales_dollar_amt_per_hour_v2', 'region__peak_returns_dollar_amt_per_hour', 'region__peak_returns_dollar_amt_per_hour_v2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = df[\"region__n_stores\"].mean()\n",
        "df[\"region__n_stores\"].fillna(f\"{a}\", inplace = True)\n"
      ],
      "metadata": {
        "id": "nfWxOTnoHWK_"
      },
      "id": "nfWxOTnoHWK_",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naCols = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
        "naCols\n",
        "\n"
      ],
      "metadata": {
        "id": "LSENGZgqHgDT",
        "outputId": "f9c2dea2-4e1d-440e-e416-17641d8d5b9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LSENGZgqHgDT",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['store__gift_returns_quantity_last_hour',\n",
              " 'region__n_open_registers',\n",
              " 'region__mean_service_time_per_customer',\n",
              " 'region__stdev_service_time_per_customer',\n",
              " 'region__sales_dollar_amt_last_hour',\n",
              " 'region__returns_dollar_amt_last_hour',\n",
              " 'region__nighttime_open_registers',\n",
              " 'region__nighttime_service_time_per_customer',\n",
              " 'region__nighttime_sales_amt_per_hour',\n",
              " 'region__nighttime_returns_amt_per_hour',\n",
              " 'region__peak_sales_dollar_amt_per_hour',\n",
              " 'region__peak_sales_dollar_amt_per_hour_v2',\n",
              " 'region__peak_returns_dollar_amt_per_hour',\n",
              " 'region__peak_returns_dollar_amt_per_hour_v2']"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for cols in naCols:\n",
        "  for col in numCols:\n",
        "    if cols == col:\n",
        "      a = df[col].mean()\n",
        "      df[col].fillna(f\"{a}\", inplace = True)\n",
        "      \n",
        "  for col in catCols:\n",
        "    if cols==col :\n",
        "       a = df[col].median()\n",
        "       df[col].fillna(f\"{a}\", inplace = True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g1rBTuCYGola"
      },
      "id": "g1rBTuCYGola",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naCols = [col for col in df.columns if df[col].isnull().sum() > 0]\n",
        "naCols\n"
      ],
      "metadata": {
        "id": "10VQz8phJXuS",
        "outputId": "fa292236-f947-429c-cb85-ba6fc32300c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "10VQz8phJXuS",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2Jf_dOhOsyq"
      },
      "id": "_2Jf_dOhOsyq",
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENCODİNG**"
      ],
      "metadata": {
        "id": "Xqj-KJ7RLKEn"
      },
      "id": "Xqj-KJ7RLKEn"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import  LabelEncoder"
      ],
      "metadata": {
        "id": "LNjrWm-RQF53"
      },
      "id": "LNjrWm-RQF53",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=[\"register__payment_types_accepted\"],drop_first=True)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['store__type_code'],drop_first=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    \n"
      ],
      "metadata": {
        "id": "bTR4ROHeLMjU"
      },
      "id": "bTR4ROHeLMjU",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=['cashier__title_level'])\n",
        "df = pd.get_dummies(df, columns=['cashier__n_years_experience'])\n",
        "df = pd.get_dummies(df, columns=['store__n_employees_total'])\n",
        "df = pd.get_dummies(df, columns=['store__n_managers'])\n",
        "df = pd.get_dummies(df, columns=['store__n_baggers'])\n",
        "df = pd.get_dummies(df, columns=['store__is_sufficiently_staffed'])\n",
        "df = pd.get_dummies(df, columns=['region__peak_sales_dollar_amt_per_hour_v2'])\n",
        "df = pd.get_dummies(df, columns=['region__peak_returns_dollar_amt_per_hour_v2'])"
      ],
      "metadata": {
        "id": "C3O20DuB2Xwg"
      },
      "id": "C3O20DuB2Xwg",
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "FUdsQXA32M75",
        "outputId": "0b2adef7-1aa7-478f-e6ee-00e41beee4fc"
      },
      "id": "FUdsQXA32M75",
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                        observation_id observation_timestamp  \\\n",
              "0           0  704d2a80-d52e-11ec-90ff-c7e6292284b3   2022-05-16 15:39:57   \n",
              "1           1  1cacc1d0-e6ac-11ec-b65d-156af70ce36b   2022-06-07 21:52:23   \n",
              "2           2  6dc2b330-d37a-11ec-884e-dfe9ea4a7bd5   2022-05-14 11:38:52   \n",
              "3           3  163ee0a0-0cca-11ed-a73c-8904b24187cc   2022-07-26 10:02:41   \n",
              "4           4  5e3c5df0-d5ee-11ec-a5f2-3b6f99e95850   2022-05-17 14:33:50   \n",
              "\n",
              "   hour_of_day  register__sales_dollar_amt_this_hour  \\\n",
              "0           15                                347.29   \n",
              "1           21                                361.59   \n",
              "2           11                                850.73   \n",
              "3           10                               1175.69   \n",
              "4           14                               3204.53   \n",
              "\n",
              "   register__peak_sales_dollar_amt_per_hour  \\\n",
              "0                                   -0.7383   \n",
              "1                                    0.6483   \n",
              "2                                   -0.4950   \n",
              "3                                   -0.5594   \n",
              "4                                    0.5693   \n",
              "\n",
              "   register__sales_dollar_amt_last_hour  register__sales_quantity_last_hour  \\\n",
              "0                               -0.1270                             -0.1993   \n",
              "1                               -0.0362                             -0.0777   \n",
              "2                               -0.1268                             -0.1974   \n",
              "3                               -0.1270                             -0.1991   \n",
              "4                               -0.1221                             -0.1632   \n",
              "\n",
              "   register__sales_quantity_rescanned_frac  \\\n",
              "0                                  -0.8299   \n",
              "1                                  -0.7395   \n",
              "2                                   1.3139   \n",
              "3                                  -0.8299   \n",
              "4                                  -0.7071   \n",
              "\n",
              "   register__sales_payments_declined_frac  ...  \\\n",
              "0                                 -0.1247  ...   \n",
              "1                                 -0.1135  ...   \n",
              "2                                  0.1075  ...   \n",
              "3                                 -0.1247  ...   \n",
              "4                                 -0.1247  ...   \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.8614  \\\n",
              "0                                                  0     \n",
              "1                                                  1     \n",
              "2                                                  0     \n",
              "3                                                  0     \n",
              "4                                                  0     \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.8559  \\\n",
              "0                                                  0     \n",
              "1                                                  0     \n",
              "2                                                  0     \n",
              "3                                                  0     \n",
              "4                                                  0     \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.8449  \\\n",
              "0                                                  0     \n",
              "1                                                  0     \n",
              "2                                                  0     \n",
              "3                                                  0     \n",
              "4                                                  0     \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.8394  \\\n",
              "0                                                  0     \n",
              "1                                                  0     \n",
              "2                                                  0     \n",
              "3                                                  0     \n",
              "4                                                  0     \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.8339  \\\n",
              "0                                                  0     \n",
              "1                                                  0     \n",
              "2                                                  0     \n",
              "3                                                  0     \n",
              "4                                                  0     \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.8284  \\\n",
              "0                                                  1     \n",
              "1                                                  0     \n",
              "2                                                  0     \n",
              "3                                                  0     \n",
              "4                                                  0     \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.7567  \\\n",
              "0                                                  0     \n",
              "1                                                  0     \n",
              "2                                                  1     \n",
              "3                                                  1     \n",
              "4                                                  0     \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_0.2356  \\\n",
              "0                                                  0    \n",
              "1                                                  0    \n",
              "2                                                  0    \n",
              "3                                                  0    \n",
              "4                                                  1    \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_1.3382  \\\n",
              "0                                                  0    \n",
              "1                                                  0    \n",
              "2                                                  0    \n",
              "3                                                  0    \n",
              "4                                                  0    \n",
              "\n",
              "   region__peak_returns_dollar_amt_per_hour_v2_-0.7567  \n",
              "0                                                  0    \n",
              "1                                                  0    \n",
              "2                                                  0    \n",
              "3                                                  0    \n",
              "4                                                  0    \n",
              "\n",
              "[5 rows x 107 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae8c899f-4926-4dc4-989e-17cacff07d5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>observation_id</th>\n",
              "      <th>observation_timestamp</th>\n",
              "      <th>hour_of_day</th>\n",
              "      <th>register__sales_dollar_amt_this_hour</th>\n",
              "      <th>register__peak_sales_dollar_amt_per_hour</th>\n",
              "      <th>register__sales_dollar_amt_last_hour</th>\n",
              "      <th>register__sales_quantity_last_hour</th>\n",
              "      <th>register__sales_quantity_rescanned_frac</th>\n",
              "      <th>register__sales_payments_declined_frac</th>\n",
              "      <th>...</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.8614</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.8559</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.8449</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.8394</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.8339</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.8284</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.7567</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_0.2356</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_1.3382</th>\n",
              "      <th>region__peak_returns_dollar_amt_per_hour_v2_-0.7567</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>704d2a80-d52e-11ec-90ff-c7e6292284b3</td>\n",
              "      <td>2022-05-16 15:39:57</td>\n",
              "      <td>15</td>\n",
              "      <td>347.29</td>\n",
              "      <td>-0.7383</td>\n",
              "      <td>-0.1270</td>\n",
              "      <td>-0.1993</td>\n",
              "      <td>-0.8299</td>\n",
              "      <td>-0.1247</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1cacc1d0-e6ac-11ec-b65d-156af70ce36b</td>\n",
              "      <td>2022-06-07 21:52:23</td>\n",
              "      <td>21</td>\n",
              "      <td>361.59</td>\n",
              "      <td>0.6483</td>\n",
              "      <td>-0.0362</td>\n",
              "      <td>-0.0777</td>\n",
              "      <td>-0.7395</td>\n",
              "      <td>-0.1135</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6dc2b330-d37a-11ec-884e-dfe9ea4a7bd5</td>\n",
              "      <td>2022-05-14 11:38:52</td>\n",
              "      <td>11</td>\n",
              "      <td>850.73</td>\n",
              "      <td>-0.4950</td>\n",
              "      <td>-0.1268</td>\n",
              "      <td>-0.1974</td>\n",
              "      <td>1.3139</td>\n",
              "      <td>0.1075</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>163ee0a0-0cca-11ed-a73c-8904b24187cc</td>\n",
              "      <td>2022-07-26 10:02:41</td>\n",
              "      <td>10</td>\n",
              "      <td>1175.69</td>\n",
              "      <td>-0.5594</td>\n",
              "      <td>-0.1270</td>\n",
              "      <td>-0.1991</td>\n",
              "      <td>-0.8299</td>\n",
              "      <td>-0.1247</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5e3c5df0-d5ee-11ec-a5f2-3b6f99e95850</td>\n",
              "      <td>2022-05-17 14:33:50</td>\n",
              "      <td>14</td>\n",
              "      <td>3204.53</td>\n",
              "      <td>0.5693</td>\n",
              "      <td>-0.1221</td>\n",
              "      <td>-0.1632</td>\n",
              "      <td>-0.7071</td>\n",
              "      <td>-0.1247</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 107 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae8c899f-4926-4dc4-989e-17cacff07d5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae8c899f-4926-4dc4-989e-17cacff07d5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae8c899f-4926-4dc4-989e-17cacff07d5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"observation_id\"], axis=1)\n"
      ],
      "metadata": {
        "id": "hWpYCJmt5tUp"
      },
      "id": "hWpYCJmt5tUp",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"observation_timestamp\"], axis=1)\n"
      ],
      "metadata": {
        "id": "D-LnsxEKTkQ5"
      },
      "id": "D-LnsxEKTkQ5",
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.drop([\"register__sales_dollar_amt_this_hour\"], axis=1)\n",
        "X = df[\"register__sales_dollar_amt_this_hour\"]"
      ],
      "metadata": {
        "id": "Pd-McCR34T5F"
      },
      "id": "Pd-McCR34T5F",
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import  RobustScaler"
      ],
      "metadata": {
        "id": "NX3Q81PDTR_T"
      },
      "id": "NX3Q81PDTR_T",
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs = RobustScaler()\n",
        "df = rs.fit_transform(df)"
      ],
      "metadata": {
        "id": "h4a6ZZUDSzKi"
      },
      "id": "h4a6ZZUDSzKi",
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "import xgboost\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor"
      ],
      "metadata": {
        "id": "DIMJviuXSZda"
      },
      "id": "DIMJviuXSZda",
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n",
        "\n",
        "m = XGBRegressor(objective='reg:squarederror').fit(X_train, y_train)\n",
        "\n",
        "#pred = m.predict(X_test)\n",
        "##rmse = np.sqrt(MSE(y_test, pred))\n",
        "#print(f\"RMSE: {round(rmse, 4)}  \")\n"
      ],
      "metadata": {
        "id": "F63sWzBKSIeM",
        "outputId": "4e9b53d8-4e21-4f97-cae5-a0c282b275bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "id": "F63sWzBKSIeM",
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-171-4cea36910a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reg:squarederror'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#pred = m.predict(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    358\u001b[0m                                    missing=self.missing, nthread=self.n_jobs)\n\u001b[1;32m    359\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mtrainDmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mevals_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    383\u001b[0m                                                             \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                                                             feature_types)\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_pandas_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_dt_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_dt_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_label\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame for label cannot have multiple columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mlabel_dtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame for label cannot have multiple columns"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44215b0a",
      "metadata": {
        "id": "44215b0a"
      },
      "source": [
        "## Weight Sampling to reduce bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "949c5317",
      "metadata": {
        "id": "949c5317"
      },
      "outputs": [],
      "source": [
        "def get_smoothing_kernel(kernel_size=5, sigma=1.0):\n",
        "    '''Builds a Gaussian smoothing kernel of a desired size (# of elements) and\n",
        "       spread. The Gaussian will be positioned at the center of the kernel.'''\n",
        "\n",
        "    # Calculate a Gaussian with mean x0 = 0 and std = sigma\n",
        "    # NOTE: Renormalize so sum(y) = 1.0\n",
        "    x = np.linspace(-(kernel_size-1) / 2., (kernel_size - 1) / 2., kernel_size)   # x-lims for Gaussian\n",
        "    kernel = np.exp(-0.5 * np.square(x) / np.square(sigma))\n",
        "    kernel = kernel / np.sum(kernel)\n",
        "\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def get_sample_weights(target_vals, target_bins,\n",
        "                       smoothing_kernel_props={'size': 51, 'sigma': 5},\n",
        "                       weighting_method='inverse_square',\n",
        "                       max_weight=100,\n",
        "                       make_plots=False):\n",
        "    '''Gets reweighting factors for each sample in a regression problem, following the\n",
        "       LDS method detailed here:\n",
        "           https://towardsdatascience.com/strategies-and-tactics-for-regression-on-imbalanced-data-61eeb0921fca\n",
        "\n",
        "       Essentially, we do these steps:\n",
        "       1.) Get a histogram of target values\n",
        "       2.) Use a Gaussian filter to smooth the histogram\n",
        "       3.) Use inverse or inverse-sqrt of counts to weight each bin\n",
        "       4.) Map each sample to its new weight\n",
        "\n",
        "       The output 'sample_weights' vector will be the same shape as 'target_vals'.\n",
        "    '''\n",
        "\n",
        "    # Get the histogram / distribution of target values -- these will be our 'classes'\n",
        "    # NOTE: Convert to float for convolution below\n",
        "    [counts, bins] = np.histogram(target_vals, bins=target_bins)\n",
        "    counts = [float(c) for c in counts]\n",
        "\n",
        "    # Apply a Gaussian smoothing filter to the counts distribution\n",
        "    # NOTE: Renormalize so the convolved distribution maintains the same # of total counts\n",
        "    kernel = get_smoothing_kernel(kernel_size=smoothing_kernel_props['size'], sigma=smoothing_kernel_props['sigma'])\n",
        "    smooth_counts = convolve1d(counts, weights=kernel, mode='mirror')\n",
        "    smooth_counts = smooth_counts * ( sum(counts) / sum(smooth_counts) )\n",
        "\n",
        "    # Make sure there are no 0-count bins. Replace them with the lowest non-zero count\n",
        "    mask = ( smooth_counts == 0 )\n",
        "    smooth_counts[mask] = np.min( smooth_counts[np.nonzero(smooth_counts)] )\n",
        "\n",
        "    # Get weights for each 'class' from the smoothed counts\n",
        "    # NOTE: Supports either 1/N or 1/N^2 reweighting\n",
        "    class_weights = np.ones(smooth_counts.shape)   # Default: No reweighting\n",
        "    if ( weighting_method == 'inverse_square' ):\n",
        "        class_weights = 1 / np.sqrt(smooth_counts)\n",
        "    if ( weighting_method == 'inverse' ):\n",
        "        class_weights = 1 / smooth_counts\n",
        "\n",
        "    # Now map each sample to the corresponding class weight\n",
        "    # NOTE: np.digitize returns the bin index used in histogramming. With k bins, we get k-1 class weights\n",
        "    idxs = np.digitize(target_vals, target_bins) - 1\n",
        "    sample_weights = class_weights[idxs]\n",
        "\n",
        "    # Rescale weights so the sum(sample_weights) = n_samples. This is what we'd get without reweighting\n",
        "    n_samples = len(target_vals)\n",
        "    scaling_factor = n_samples / sum(sample_weights)\n",
        "    sample_weights = sample_weights * scaling_factor\n",
        "\n",
        "    # Truncate very large weights to reduce noise (especially helpful for 'inverse' weighting mode)\n",
        "    sample_weights = np.clip(sample_weights, 0, max_weight)\n",
        "\n",
        "    # (OPTIONAL) Make plots of both the weights & the target distributions (smoothed & unsmoothed)\n",
        "    if ( make_plots ):\n",
        "\n",
        "        # Plot 1: Target distribution (smoothed vs. unsmoothed)\n",
        "        fig = plt.figure(figsize=[8,4])\n",
        "        ax = fig.gca()\n",
        "        plt.plot(target_bins[1:], counts)\n",
        "        plt.plot(target_bins[1:], smooth_counts, color='orange')\n",
        "        plt.title('Class Distributions', size=20)\n",
        "        plt.xlabel('Target Value', size=16)\n",
        "        plt.ylabel('Counts', size=16)\n",
        "        ax.tick_params(axis='both', labelsize=14)\n",
        "        plt.xlim([target_bins[0], target_bins[-1]])\n",
        "        plt.ylim([0,None])\n",
        "        plt.grid('minor')\n",
        "        plt.legend(['Original', 'Smoothed'], fontsize=14)\n",
        "\n",
        "        # Plot 2: Class weights\n",
        "        fig = plt.figure(figsize=[8,4])\n",
        "        ax = fig.gca()\n",
        "        plt.plot(target_bins[1:], np.clip(class_weights * scaling_factor, 0, max_weight))\n",
        "        plt.hlines([1], target_bins[0], target_bins[-1], colors='orange')\n",
        "        plt.title('Class Weights', size=20)\n",
        "        plt.xlabel('Target Value', size=16)\n",
        "        plt.ylabel('Weight', size=16)\n",
        "        ax.tick_params(axis='both', labelsize=14)\n",
        "        plt.xlim([target_bins[0], target_bins[-1]])\n",
        "        plt.ylim([0,None])\n",
        "        plt.grid('minor')\n",
        "        plt.legend(['Reweighted', 'Unweighted'], fontsize=14)\n",
        "\n",
        "    return sample_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba8fcde",
      "metadata": {
        "id": "8ba8fcde"
      },
      "source": [
        "## Define Hyperparameter Tuning parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "c842542e",
      "metadata": {
        "id": "c842542e"
      },
      "outputs": [],
      "source": [
        "HYPERPARAMETER_DTYPES = {\n",
        "    'n_estimators': int,\n",
        "    'max_depth': int,\n",
        "    'learning_rate': float\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Heperparameter tuning function"
      ],
      "metadata": {
        "id": "RgSKIdMk0p8p"
      },
      "id": "RgSKIdMk0p8p"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "3aedee25",
      "metadata": {
        "id": "3aedee25"
      },
      "outputs": [],
      "source": [
        "def random_search_xgb(hyperparameter_space, X_train, y_train,\n",
        "                      wt_train=None,\n",
        "                      metric_to_optimize='r2',\n",
        "                      n_runs=100,\n",
        "                      n_folds=5,\n",
        "                      n_jobs=4,\n",
        "                      print_status=True):\n",
        "    '''Performs random search hyperparameter tuning for an XGBoost model. We'll evaluate model\n",
        "       performance at many random points in the hyperparameter space, choosing the best result\n",
        "       that was found. Supports sample reweighting via 'wt_train'.\n",
        "\n",
        "       Parameters are optimized using cross-validation of a specified model evaluation metric:\n",
        "           -- 'r2'\n",
        "           -- 'rmse'\n",
        "           -- 'mae'\n",
        "           -- 'mape'\n",
        "           -- 'racc'\n",
        "       This last one is the 'regression accuracy' at 10% maximum error, i.e. the number\n",
        "       of predictions that are within 10% of true.\n",
        "\n",
        "       Each key in the hyperparameter_space dictionary should be a named hyperparameter,\n",
        "       while values should be arrays containing the min/max values to test.\n",
        "\n",
        "       Returns the trained optimal model, along with performance metrics and the optimal\n",
        "       hyperparameters used.\n",
        "\n",
        "       NOTE: Currently only support 'n_estimators', 'max_depth', and 'learning_rate'.'''\n",
        "\n",
        "    t0 = time()\n",
        "\n",
        "    # Initialize the return values\n",
        "    optimal_model = None\n",
        "    optimal_hyperparameters = {}\n",
        "    optimal_eval_metrics = None\n",
        "\n",
        "    # Print status message\n",
        "    if ( print_status ):\n",
        "        print('XGBoost Hyperparameter Search')\n",
        "        print('-'*40)\n",
        "        print('Method:       {0:s}'.format('random-search'))\n",
        "        print('Dataset:      {0:d} pts x {1:d} dimensions'.format(X_train.shape[0], X_train.shape[1]))\n",
        "        print('Optimizer:    {0:s}'.format(metric_to_optimize.upper()))\n",
        "        print('# of Runs:    {0:d}'.format(n_runs))\n",
        "        print()\n",
        "        print('Hyperparameter Space:')\n",
        "        for param in hyperparameter_space.keys():\n",
        "            print('-- {0:<16s} [{1:s}]'.format(param+':', ', '.join([str(val) for val in hyperparameter_space[param]]) ))\n",
        "        print('\\n')\n",
        "\n",
        "    # Try a bunch of random sets of hyperparameters, returning results for the best one\n",
        "    optimal_metric_val = None\n",
        "    n_evals = 0\n",
        "    for run_num in range(n_runs):\n",
        "\n",
        "        # Pick some random hyperparameters to start with\n",
        "        current_hyperparameters = get_random_hyperparameters(hyperparameter_space)\n",
        "        current_metric_val = None\n",
        "\n",
        "        # (OPTIONAL) Print progress\n",
        "        if ( print_status ):\n",
        "            format_str = 'Progress: Run {0:>3s} / {1:d}   -->   Hyperparameters: {2:s}' + ' '*80\n",
        "            print(format_str.format('#{0:d}'.format(run_num+1), n_runs, str(current_hyperparameters)), end='\\r', flush=True)\n",
        "\n",
        "        # Evaluate an XGBoost Regressor with the current hyperparameters\n",
        "        current_model = initialize_model_xgb(current_hyperparameters)\n",
        "        [current_eval_results, _] = evaluate_regression_model(current_model, X_train=X_train, y_train=y_train, wt_train=wt_train,\n",
        "                                                              n_folds=n_folds, n_jobs=n_jobs)\n",
        "        n_evals += 1\n",
        "\n",
        "        # Compare model performance w/ best found so far\n",
        "        current_metric_val = current_eval_results[metric_to_optimize]['mean']\n",
        "        if ( ( optimal_metric_val is None ) or ( current_metric_val > optimal_metric_val ) ):\n",
        "            optimal_metric_val = current_metric_val\n",
        "            optimal_hyperparameters = current_hyperparameters\n",
        "            optimal_eval_results = current_eval_results\n",
        "\n",
        "\n",
        "    # Train the final optimized model\n",
        "    optimal_model = train_model_xgb(optimal_hyperparameters, X_train, y_train, wt_train=wt_train)\n",
        "\n",
        "    # (OPTIONAL) Print summary\n",
        "    if ( print_status ):\n",
        "        print('\\n\\n')\n",
        "        print('Search Complete')\n",
        "        print('-'*40)\n",
        "        print('Optimal Params:      {0:s}'.format(str(optimal_hyperparameters)))\n",
        "        print('Model Performance:   {0:s} = {1:3.4f}'.format(metric_to_optimize, optimal_metric_val))\n",
        "        print('Params Evaluated:    {0:d}'.format(n_evals))\n",
        "        print('Execution Time:      {0:3.2f}s'.format(time()-t0))\n",
        "        print()\n",
        "\n",
        "    return optimal_model, optimal_hyperparameters, optimal_eval_results\n",
        "\n",
        "\n",
        "def get_random_hyperparameters(hyperparameter_space):\n",
        "    '''Randomly selects a point in hyperparameter space. Outputs will fall within the\n",
        "       provided ranges'''\n",
        "\n",
        "    # For each hyperparameter, pick a random value somewhere within the provided range\n",
        "    hyperparameters = {}\n",
        "    for param in hyperparameter_space.keys():\n",
        "        min_value = min(hyperparameter_space[param])\n",
        "        max_value = max(hyperparameter_space[param])\n",
        "        hyperparameters[param] = min_value + (max_value - min_value) * np.random.random()\n",
        "\n",
        "        # Convert to desired datatype\n",
        "        if ( HYPERPARAMETER_DTYPES[param] == int ):\n",
        "            hyperparameters[param] = int(np.round(hyperparameters[param]))\n",
        "\n",
        "    return hyperparameters\n",
        "\n",
        "\n",
        "def initialize_model_xgb(hyperparameters):\n",
        "    '''Initializes an XGBoost Regressor w/ desired hyperparameters'''\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor(n_estimators=int(hyperparameters['n_estimators']),\n",
        "                                 max_depth=int(hyperparameters['max_depth']),\n",
        "                                 learning_rate=hyperparameters['learning_rate'])\n",
        "\n",
        "    return xgb_model\n",
        "\n",
        "\n",
        "def train_model_xgb(hyperparameters, X_train, y_train, wt_train=None):\n",
        "    '''Trains an XGBoost Regressor with a desired set of hyperparameters. Supports\n",
        "       sample reweighting via 'wt_train'.'''\n",
        "\n",
        "    # Initialize the model w/ desired hyperparameters\n",
        "    xgb_model = initialize_model_xgb(hyperparameters)\n",
        "\n",
        "    # Fit to training data\n",
        "    xgb_model.fit(X_train, y_train, sample_weight=wt_train)\n",
        "\n",
        "    return xgb_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f0666a",
      "metadata": {
        "id": "69f0666a"
      },
      "source": [
        "## Model Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "ffa7fe57",
      "metadata": {
        "id": "ffa7fe57"
      },
      "outputs": [],
      "source": [
        "def evaluate_regression_model(regression_model,\n",
        "                              X_train=None,\n",
        "                              y_train=None,\n",
        "                              wt_train=None,\n",
        "                              X_test=None,\n",
        "                              y_test=None,\n",
        "                              wt_test=None,\n",
        "                              y_lims=[None,None],\n",
        "                              n_folds=5,\n",
        "                              n_jobs=4):\n",
        "    '''Evaluates a regression model on a set of common performance metrics. Cross-validation\n",
        "       is used to evaluate the training set (X_train, y_train), resulting in multiple values\n",
        "       per metric. For the test set (X_test, y_test), metrics are computed on the model's\n",
        "       predictions, resulting in a single value per metric. Supports any model architecture\n",
        "       with *.fit() and *.predict() methods.\n",
        "\n",
        "       Supports sample reweighting via 'wt_train'.\n",
        "\n",
        "       NOTE: User has the option to coerce predictions within the range given by y_lims.'''\n",
        "\n",
        "    # Initialize return values\n",
        "    eval_results__train = {}\n",
        "    eval_results__test = {}\n",
        "\n",
        "    # If sample weights are provided, package them as a kwarg for cross_validate()\n",
        "    fit_params = None\n",
        "    if ( wt_train is not None ):\n",
        "        fit_params = {'sample_weight': wt_train}\n",
        "\n",
        "    # If a training set is provided, run cross-validation on it\n",
        "    if ( ( X_train is not None ) and ( y_train is not None ) ):\n",
        "\n",
        "        # Cross-validate model performance\n",
        "        eval_metrics = {\n",
        "            'r2': make_scorer(r2_score),\n",
        "            'rmse': make_scorer(rmse),\n",
        "            'mae': make_scorer(mae),\n",
        "            'mape': make_scorer(mape),\n",
        "            'racc': make_scorer(regression_accuracy)\n",
        "        }\n",
        "        eval_results = cross_validate(regression_model, X_train, y_train, fit_params=fit_params,\n",
        "                                      cv=n_folds, scoring=eval_metrics, n_jobs=n_jobs)\n",
        "\n",
        "        # Let's compute means & standard dev's for all metrics, and change the output names\n",
        "        # NOTE: Cross-validation above gives the metric names a 'test_' prefix\n",
        "        output_name_mappings = {\n",
        "            'fit_time': 'fit_time',\n",
        "            'score_time': 'score_time',\n",
        "            'test_r2': 'r2',\n",
        "            'test_rmse': 'rmse',#\n",
        "            'test_mae': 'mae',\n",
        "            'test_mape': 'mape',\n",
        "            'test_racc': 'racc'\n",
        "        }\n",
        "        for metric in eval_results.keys():\n",
        "            metric_ = output_name_mappings[metric]\n",
        "            eval_results__train[metric_] = {\n",
        "                'mean': np.mean(eval_results[metric]),\n",
        "                'std': np.std(eval_results[metric]),\n",
        "                'data': eval_results[metric]\n",
        "            }\n",
        "\n",
        "    # If a test set is provided, run evaluation on the predictions\n",
        "    if ( ( X_test is not None ) and ( y_test is not None ) ):\n",
        "\n",
        "        # Get measured & predicted results for the test dataset\n",
        "        y_meas = y_test\n",
        "        y_pred = regression_model.predict(X_test)\n",
        "\n",
        "        # (OPTIONAL) Coerce predictions into allowed range (e.x. non-negative)\n",
        "        if ( y_lims[0] is not None ):\n",
        "            mask = y_pred < y_lims[0]\n",
        "            y_pred[mask] = y_lims[0]\n",
        "        if ( y_lims[1] is not None ):\n",
        "            mask = y_pred > y_lims[1]\n",
        "            y_pred[mask] = y_lims[1]\n",
        "\n",
        "        # Compute the desired performance metrics\n",
        "        eval_results__test = {\n",
        "            'data': {'y_meas': y_meas, 'y_pred': y_pred},\n",
        "            'r2': r2_score(y_meas, y_pred),\n",
        "            'rmse': rmse(y_meas, y_pred),\n",
        "            'mae': mae(y_meas, y_pred),\n",
        "            'mape': mape(y_meas, y_pred),\n",
        "            'racc': regression_accuracy(y_meas, y_pred)\n",
        "        }\n",
        "    return eval_results__train, eval_results__test\n",
        "\n",
        "\n",
        "def regression_accuracy(y_meas, y_pred, max_error=20, error_type='relative'):\n",
        "    '''Compares predicted & measured values, returning the percentage of predictions\n",
        "       that are within a set error limit. This error limit can be an absolute value\n",
        "       or a relative percentage'''\n",
        "\n",
        "    # OPTION 1: Relative percentage\n",
        "    if ( error_type == 'relative' ):\n",
        "        mask = 100.0 * abs((y_pred - y_meas) / y_meas) < max_error\n",
        "\n",
        "    # OPTION 2: Absolute value\n",
        "    if ( error_type == 'absolute' ):\n",
        "        mask = abs(y_pred - y_meas) < max_error\n",
        "\n",
        "    accuracy = sum(mask) / len(mask)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def rmse(y_meas, y_pred):\n",
        "    '''Computes RMSE for predicted vs. measured data points'''\n",
        "\n",
        "    n_samples = len(y_meas)\n",
        "    rmse = np.linalg.norm(y_pred - y_meas) / np.sqrt(n_samples)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "\n",
        "def mae(y_meas, y_pred):\n",
        "    '''Computes MAE for predicted vs. measured data points'''\n",
        "\n",
        "    n_samples = len(y_meas)\n",
        "    mae = np.sum(np.abs(y_pred - y_meas)) / n_samples\n",
        "\n",
        "    return mae\n",
        "\n",
        "\n",
        "def mape(y_meas, y_pred):\n",
        "    '''Computes MAPE for predicted vs. measured data points'''\n",
        "\n",
        "    n_samples = len(y_meas)\n",
        "    mape = np.sum(np.abs((y_pred - y_meas)/y_meas)) / n_samples\n",
        "\n",
        "    return mape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5b1285",
      "metadata": {
        "id": "0d5b1285"
      },
      "source": [
        "## Function to  visualize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "7a366855",
      "metadata": {
        "id": "7a366855"
      },
      "outputs": [],
      "source": [
        "# Define a few standard color palettes\n",
        "color_palettes = {\n",
        "\n",
        "    'tableau': [\n",
        "        'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple',\n",
        "        'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'\n",
        "    ],\n",
        "\n",
        "    'blues': ['darkblue', 'blue', 'slateblue', 'rebeccapurple', 'indigo'],\n",
        "\n",
        "    'oranges': ['darkorange', 'orange', 'sandybrown', 'peru', 'saddlebrown']\n",
        "\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "c5f5ef9d",
      "metadata": {
        "id": "c5f5ef9d"
      },
      "outputs": [],
      "source": [
        "def plot_distribution(data,\n",
        "                      data_labels=None,\n",
        "                      bins=None,\n",
        "                      title='Distribution',\n",
        "                      xlabel='Variable',\n",
        "                      hist_units='counts',\n",
        "                      output_filepath=None):\n",
        "    '''Plots the probability distribution of one or more variables. User can choose\n",
        "       output in counts/bin or probability/bin.'''\n",
        "\n",
        "    # Define default bin structure if not provided\n",
        "    if ( bins is None ):\n",
        "        n_bins = 50\n",
        "        min_val = None\n",
        "        max_val = None\n",
        "        for dataset in data:\n",
        "            this_min = np.nanmin(dataset)\n",
        "            this_max = np.nanquantile(dataset, 0.99) #np.nanmax(dataset)\n",
        "            if ( min_val is None ):\n",
        "                min_val = this_min\n",
        "            elif ( this_min < min_val ):\n",
        "                min_val = this_min\n",
        "            if ( max_val is None ):\n",
        "                max_val = this_min\n",
        "            elif ( this_max > max_val ):\n",
        "                max_val = this_max\n",
        "        bins = np.linspace(min_val, max_val, n_bins)\n",
        "\n",
        "    # Set up plot\n",
        "    fig = plt.figure(figsize=[10,8], facecolor='w')\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # Plot a histogram for each input data series\n",
        "    # NOTE: Normalize to probability per bin if desired\n",
        "    for dataset in data:\n",
        "        [counts,bins] = np.histogram(dataset, bins=bins)\n",
        "        counts_ = np.append([0], counts)\n",
        "        if ( hist_units == 'probability' ):\n",
        "            counts_ = counts_ / sum(counts_)\n",
        "        plt.step(bins,counts_)\n",
        "\n",
        "    # Set up plot properties\n",
        "    plt.title(title, size=24)\n",
        "    plt.xlabel(xlabel, size=20)\n",
        "    if ( hist_units == 'probability' ):\n",
        "        plt.ylabel('Probability Density\\n(1/Bin)', size=20)\n",
        "    if ( hist_units == 'counts' ):\n",
        "        plt.ylabel('Counts\\n(#/Bin)', size=20)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.xlim([bins[0],bins[-1]])\n",
        "    plt.ylim([0,None])\n",
        "    plt.grid('minor')\n",
        "\n",
        "    # Set up legend\n",
        "    if ( data_labels is not None ):\n",
        "        ax.legend(data_labels, fontsize=16, shadow=True)\n",
        "        #bbox_to_anchor = [1.05,0.5]\n",
        "        #ax.legend(data_labels, loc='center left', fontsize=16, shadow=True, bbox_to_anchor=bbox_to_anchor)\n",
        "\n",
        "    # (OPTIONAL) Save figure if desired\n",
        "    if ( output_filepath is not None ):\n",
        "        plt.savefig(output_filepath, bbox_inches='tight')\n",
        "    plt.show()\n",
        "#\n",
        "\n",
        "def plot_heatmap(x, y,\n",
        "                 x_bins=None,\n",
        "                 y_bins=None,\n",
        "                 aspect='equal',\n",
        "                 scale='log',\n",
        "                 title='Heatmap',\n",
        "                 xlabel='Variable 1',\n",
        "                 ylabel='Variable 2',\n",
        "                 cmap='Reds',\n",
        "                 plot_diagonal=False,\n",
        "                 output_filepath=None):\n",
        "    '''Renders a heatmap of y vs. x. Generally produces a clearer visualization\n",
        "       than a simple scatterplot'''\n",
        "\n",
        "    # Define default bin structures if they weren't provided\n",
        "    if ( x_bins is None ):\n",
        "        x_bins = np.linspace(min(x), max(x), 100)\n",
        "    if ( y_bins is None ):\n",
        "        y_bins = np.linspace(min(y), max(y), 100)\n",
        "\n",
        "    # Bin the data into a 2D histogram\n",
        "    [counts, x_bins, y_bins] = np.histogram2d(x, y, bins=[x_bins, y_bins])\n",
        "    counts = np.transpose(counts)\n",
        "\n",
        "    # Set up colormap scale\n",
        "    # NOTE: Enforce 'linear' scale if counts are too low\n",
        "    if ( np.max(counts) < 10 ):\n",
        "        scale = 'linear'\n",
        "    color_norm = None\n",
        "    if ( scale == 'log' ):\n",
        "        color_norm = colors.LogNorm(vmin=1, vmax=counts.max())\n",
        "        # If using log-scale, set 0-count bins to 1. Otherwise we'll have missing pixels in the heatmap\n",
        "        mask = counts == 0\n",
        "        counts[mask] = 1\n",
        "\n",
        "    # Render a heatmap from the counts, using bin limits to define the extent of the figure\n",
        "    fig = plt.figure(figsize=[12.5,8], facecolor='w')\n",
        "    ax = plt.gca()\n",
        "    cb = plt.imshow(counts, cmap=cmap, norm=color_norm,\n",
        "                    origin='lower', aspect=aspect, interpolation='bilinear',\n",
        "                    extent=[x_bins[0],x_bins[-1],y_bins[0],y_bins[-1]])\n",
        "\n",
        "    # Set up plot properties\n",
        "    plt.title(title, size=24)\n",
        "    plt.xlabel(xlabel, size=20)\n",
        "    plt.ylabel(ylabel, size=20)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.grid('minor')\n",
        "    fig.colorbar(cb, ax=ax)\n",
        "    bbox_props = dict(facecolor='w', edgecolor='k')\n",
        "\n",
        "    # (OPTIONAL) Plot a dashed line along the diagonal\n",
        "    if ( plot_diagonal ):\n",
        "        xlims = ax.get_xlim()\n",
        "        ylims = ax.get_ylim()\n",
        "        ax.plot(xlims, ylims, linestyle='--', linewidth=1, color=[0.25,0.25,0.25])\n",
        "\n",
        "    # (OPTIONAL) Save figure if desired\n",
        "    if ( output_filepath is not None ):\n",
        "        plt.savefig(output_filepath, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "#\n",
        "def plot_regression_accuracy(y_meas, y_pred,\n",
        "                             min_error=0.0,\n",
        "                             max_error=100.0,\n",
        "                             error_type='relative',\n",
        "                             title='Model Accuracy',\n",
        "                             xlabel='Maximum Error\\n(%)',\n",
        "                             ylabel='Accuracy\\n(%)',\n",
        "                             output_filepath=None):\n",
        "    '''Plot classification accuracy of a regression model as a function of maximum\n",
        "       allowed error. For example, if we require predictions to be within +/- 10% of\n",
        "       the true measured answer, then we can convert continuous regression output to\n",
        "       a simple Y/N answer. Was the prediction within 10%?\n",
        "\n",
        "       Calculates accuracy for increasingly permissive errors. A highly-performant\n",
        "       model will have most samples fall within a small regression error.'''\n",
        "\n",
        "    # Calculate accuracy as a function of regression error\n",
        "    n_bins = 100\n",
        "    error_limits = np.linspace(min_error, max_error, n_bins)\n",
        "    accuracies = np.zeros(n_bins)\n",
        "    for n in range(n_bins):\n",
        "        accuracies[n] = regression_accuracy(y_meas, y_pred, max_error=error_limits[n], error_type=error_type)\n",
        "\n",
        "    # Set up plot\n",
        "    fig = plt.figure(figsize=[8,8], facecolor='w')\n",
        "    ax = plt.gca()\n",
        "    plt.plot(error_limits, 100*accuracies)\n",
        "\n",
        "    # Set up plot properties\n",
        "    plt.title(title, size=24)\n",
        "    plt.xlabel(xlabel, size=20)\n",
        "    plt.ylabel(ylabel, size=20)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.xlim([error_limits[0], error_limits[-1]])\n",
        "    plt.ylim([0,100])\n",
        "    plt.grid('minor')\n",
        "\n",
        "    # (OPTIONAL) Save figure if desired\n",
        "    if ( output_filepath is not None ):\n",
        "        plt.savefig(output_filepath, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9560acb",
      "metadata": {
        "id": "a9560acb"
      },
      "source": [
        "## Define function to Load The data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "f600fd87",
      "metadata": {
        "id": "f600fd87"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_columns', None)\n",
        "output_path = '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define function to load dataset"
      ],
      "metadata": {
        "id": "R-DATrr801sh"
      },
      "id": "R-DATrr801sh"
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "f9339ce8",
      "metadata": {
        "id": "f9339ce8"
      },
      "outputs": [],
      "source": [
        "def load_dataset(input_filepath, dataset_name=None, datetime_cols=None):\n",
        "    '''Loads a *.txt file into a Pandas DataFrame'''\n",
        "    \n",
        "    t0 = time()\n",
        "    df = pd.read_csv(input_filepath, sep=',', parse_dates=datetime_cols, infer_datetime_format=True)\n",
        "\n",
        "    # Print status message\n",
        "    print('Dataset Summary')\n",
        "    print('----------------------------')\n",
        "    print('Name:    {0:s}'.format(str(dataset_name)))\n",
        "    print('Records: {0:d}'.format(df.shape[0]))\n",
        "    print('Columns: {0:d}'.format(df.shape[1]))\n",
        "    print('Size:    {0:3.2f}MB'.format(df.memory_usage(deep=True).sum() / 1000000))\n",
        "    print('Time:    {0:3.2f}s'.format(time() - t0))\n",
        "    print()\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15bb3eb2",
      "metadata": {
        "id": "15bb3eb2"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "86699e88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "86699e88",
        "outputId": "e5daa3db-f8f4-4c67-80e5-5b90c0574330"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-400586698dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/training_dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observation_timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-25a5d7413053>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(input_filepath, dataset_name, datetime_cols)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Print status message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/training_dataset.csv'"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Load input datasets\n",
        "################################################################################\n",
        "\n",
        "# Train dataset\n",
        "input_filepath = '/content/training_dataset.csv'\n",
        "train_df = load_dataset(input_filepath, dataset_name='train', datetime_cols=['observation_timestamp'])\n",
        "\n",
        "# Test dataset\n",
        "input_filepath = '/content/test_dataset.csv'\n",
        "test_df = load_dataset(input_filepath, dataset_name='test', datetime_cols=['observation_timestamp'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0395122b",
      "metadata": {
        "id": "0395122b"
      },
      "source": [
        "## Split the training data into Training and Validation(test) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b7352f",
      "metadata": {
        "id": "58b7352f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "train_df,valid_df  = train_test_split(train_df,test_size=0.2,random_state=42)\n",
        "print(f\"train dataset shape :{train_df.shape}\")\n",
        "print(f\"validation dataset shape : {valid_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28489fd2",
      "metadata": {
        "id": "28489fd2"
      },
      "source": [
        "## Defining Features and Targets\n",
        "\n",
        "This is just for reference, competitors are free chose the features of their and experiment further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4eb6c3c",
      "metadata": {
        "id": "b4eb6c3c"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Define training target & feature columns\n",
        "# NOTE: All columns will need to be numeric to work with sklearn or XGBoost! \n",
        "################################################################################\n",
        "\n",
        "# Target: What we're trying to predict\n",
        "target_col = 'register__sales_dollar_amt_this_hour'\n",
        "target_units = '$/hr'\n",
        "target_label = 'Register Sales Per Hour'\n",
        "target_range = [0,5000]\n",
        "\n",
        "# Index: What uniquely determines each record?\n",
        "index_col = 'observation_id'\n",
        "\n",
        "# Features: What we're using to predict the target\n",
        "# NOTE: See the data dictionary for a complete list of available features\n",
        "feature_cols = [\n",
        "    'store__n_open_registers', \n",
        "    'cashier__item_scan_rate_per_min',\n",
        "    'register__peak_sales_dollar_amt_per_hour',\n",
        "    'register__sales_quantity_rescanned_frac',\n",
        "    'store__occupancy_outdoors',\n",
        "    'region__n_open_registers',\n",
        "    'region__peak_sales_dollar_amt_per_hour',\n",
        "    #'region__peak_sales_dollar_amt_per_hour_v2',\n",
        "    'hour_of_day'\n",
        "]\n",
        "\n",
        "# Metadata: Not used in model training, but kept to tag the data\n",
        "metadata_cols = ['observation_timestamp']\n",
        "\n",
        "# Unused: All other columns (e.x. Unusable or unknown columns)\n",
        "available_cols = train_df.columns.values\n",
        "used_cols = [index_col, target_col] + feature_cols + metadata_cols\n",
        "unused_cols = [col for col in available_cols if col not in used_cols]\n",
        "\n",
        "# Print summary stats for the target variable\n",
        "target_mean = train_df[target_col].mean()\n",
        "target_std = train_df[target_col].std()\n",
        "target_min = train_df[target_col].min()\n",
        "target_max = train_df[target_col].max()\n",
        "target_quantiles = train_df[target_col].quantile([0.1, 0.25, 0.5, 0.75, 0.9]).values\n",
        "target_iqr = target_quantiles[3] - target_quantiles[1]\n",
        "print('Target Variable: {0:s}'.format(target_col))\n",
        "print('----------------------------------------')\n",
        "print('Units:  {0:s}'.format(target_units))\n",
        "print('Mean:   {0:3.2f} +/- {1:3.2f}'.format(target_mean, target_std))\n",
        "print('IQR:    {0:3.2f}'.format(target_iqr))\n",
        "print()\n",
        "print('|---------+---------+---------+---------+---------+---------+---------|')\n",
        "print('|   Min   |   10%   |   25%   |   Med   |   75%   |   90%   |   Max   |')\n",
        "print('|---------+---------+---------+---------+---------+---------+---------|')\n",
        "print('|{0:^9.2f}|{1:^9.2f}|{2:^9.2f}|{3:^9.2f}|{4:^9.2f}|{5:^9.2f}|{6:^9.2f}|'.format(target_min, target_quantiles[0], target_quantiles[1], target_quantiles[2], target_quantiles[3], target_quantiles[4], target_max))\n",
        "print('|---------+---------+---------+---------+---------+---------+---------|')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73158ec7",
      "metadata": {
        "id": "73158ec7"
      },
      "source": [
        "## Data Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d19ef7d",
      "metadata": {
        "id": "2d19ef7d"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Do some basic transformations\n",
        "################################################################################\n",
        "\n",
        "# This will only include one-hot encoding & NULL-imputation here.\n",
        "# Let's transform each input dataset separately\n",
        "categorical_cols = ['register__payment_types_accepted', 'store__type_code']\n",
        "imputation_cols = [col for col in feature_cols if col not in categorical_cols]\n",
        "\n",
        "# Dataset: train\n",
        "print('Transforming \\'train\\' dataset...')\n",
        "train_df = encode_categoricals(train_df, categorical_cols)\n",
        "[train_df, imputation_mapping] = impute_nulls(train_df, imputation_cols)\n",
        "print()\n",
        "\n",
        "# Dataset: test\n",
        "print('Transforming \\'test\\' dataset...')\n",
        "test_df = encode_categoricals(test_df, categorical_cols)\n",
        "[test_df, imputation_mapping] = impute_nulls(test_df, imputation_cols, imputation_mapping=imputation_mapping)\n",
        "print()\n",
        "\n",
        "# Dataset: validation\n",
        "print('Transforming \\'validation\\' dataset...')\n",
        "valid_df = encode_categoricals(valid_df, categorical_cols)\n",
        "[valid_df, imputation_mapping] = impute_nulls(valid_df, imputation_cols, imputation_mapping=imputation_mapping)\n",
        "print()\n",
        "\n",
        "# Print status\n",
        "print()\n",
        "print('Transformations complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0591459",
      "metadata": {
        "id": "c0591459"
      },
      "source": [
        "## Re-weight training samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d9536f",
      "metadata": {
        "id": "b6d9536f"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Reweight training samples to reduce bias & improve performance where we have\n",
        "# less data available. The resulting model should generalize somewhat better\n",
        "################################################################################\n",
        "\n",
        "target_vals = train_df[target_col].values\n",
        "target_bins = np.linspace(target_range[0],target_range[1],501)\n",
        "smoothing_kernel_props = {'size': 51, 'sigma': 7}\n",
        "sample_weights = get_sample_weights(target_vals, target_bins, \n",
        "                                    smoothing_kernel_props=smoothing_kernel_props, \n",
        "                                    weighting_method='inverse_square', \n",
        "                                    max_weight=10,\n",
        "                                    make_plots=True)\n",
        "train_df['sample_weights'] = sample_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f25300f",
      "metadata": {
        "id": "2f25300f"
      },
      "source": [
        "## Build training & validation Datasests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2032c19",
      "metadata": {
        "id": "f2032c19"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Build train/test datasets that are suitable as modeling inputs\n",
        "################################################################################\n",
        "\n",
        "# Extract features, targets, and (optionally) weights\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df[target_col].values\n",
        "wt_train = train_df['sample_weights'].values\n",
        "\n",
        "X_test = test_df[feature_cols].values\n",
        "#y_test = test_df[target_col].values # Commented out as y_test does not exist in the data\n",
        "\n",
        "X_valid = valid_df[feature_cols].values\n",
        "y_valid = valid_df[target_col].values\n",
        "\n",
        "print('Dataset Summary')\n",
        "print('-'*40)\n",
        "print('Train:         {0:>5d} samples  x  {1:d} features '.format(X_train.shape[0], X_train.shape[1]))\n",
        "print('Test:          {0:>5d} samples  x  {1:d} features '.format(X_test.shape[0], X_test.shape[1]))\n",
        "print('Validation:    {0:>5d} samples  x  {1:d} features '.format(X_valid.shape[0], X_valid.shape[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "439f9ec3",
      "metadata": {
        "id": "439f9ec3"
      },
      "source": [
        "## Build the XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21755f63",
      "metadata": {
        "id": "21755f63"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Train an XGBoost model w/ hyperparameter tuning \n",
        "################################################################################\n",
        "\n",
        "# Set up the ranges of hyperparameters to test\n",
        "optimize_hyperparameters = False\n",
        "n_runs = 250    # Number of pts to test during random search optimization\n",
        "hyperparameter_space = {\n",
        "    'n_estimators': [10,75],\n",
        "    'learning_rate': [0.05,0.5],\n",
        "    'max_depth': [1,4]\n",
        "}\n",
        "default_hyperparameters = {'n_estimators': 64, 'learning_rate': 0.2551, 'max_depth': 5}    # Used when tuning is skipped\n",
        "metric_to_optimize = 'racc'    # Options: ['rmse', 'r2', 'mae', 'mape', 'racc']\n",
        "\n",
        "# (OPTIONAL) Optimize hyperparameters via random search, if desired\n",
        "if ( optimize_hyperparameters ):\n",
        "    [xgb_model, hyperparameters, _] = random_search_xgb(hyperparameter_space, \n",
        "                                                        X_train, \n",
        "                                                        y_train, \n",
        "                                                        wt_train=wt_train,\n",
        "                                                        n_runs=n_runs,\n",
        "                                                        metric_to_optimize=metric_to_optimize)\n",
        "\n",
        "# Otherwise, just train a model on the default parameters\n",
        "else:\n",
        "    hyperparameters = default_hyperparameters\n",
        "    xgb_model = train_model_xgb(hyperparameters, X_train, y_train, wt_train=wt_train)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c7af62",
      "metadata": {
        "id": "85c7af62"
      },
      "source": [
        "## Evaluate the performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97512342",
      "metadata": {
        "id": "97512342"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Evaluate model performance ('validation' dataset)\n",
        "# NOTE: Make sure 'valid_df' is already loaded & the data has been transformed!\n",
        "################################################################################\n",
        "\n",
        "# Evaluate model performance on both train/validation datasets\n",
        "[eval_results__train, eval_results__test] = evaluate_regression_model(xgb_model, \n",
        "                                                                      X_train=X_train, y_train=y_train, wt_train=wt_train, \n",
        "                                                                      X_test=X_valid, y_test=y_valid, \n",
        "                                                                      y_lims=[0,None])\n",
        "y_meas = eval_results__test['data']['y_meas']\n",
        "y_pred = eval_results__test['data']['y_pred']\n",
        "r2 = eval_results__test['r2']\n",
        "result_df = pd.DataFrame({'prediction':y_pred})\n",
        "print(result_df.head())\n",
        "result_df.to_csv('validation_results.csv', index=False)\n",
        "# Print a performance summary\n",
        "n_samples__train = X_train.shape[0]\n",
        "n_samples__test = X_valid.shape[0]\n",
        "print('Model Performance Summary')\n",
        "print('Target: {0:s} ({1:s})'.format(target_label, target_units))\n",
        "print('-'*40)\n",
        "print('Hyperparameters')\n",
        "print('-- {0:<16s} {1:d}'.format('n_estimators:', hyperparameters['n_estimators']))\n",
        "print('-- {0:<16s} {1:d}'.format('max_depth:', hyperparameters['max_depth']))\n",
        "print('-- {0:<16s} {1:3.3f}'.format('learning_rate:', hyperparameters['learning_rate']))\n",
        "print('\\nModel Performance (Train)')\n",
        "print('# of Samples: {0:d}'.format(n_samples__train))\n",
        "print('-- R^2:  {0:>8.4f} +/- {1:3.4f}'.format(eval_results__train['r2']['mean'], eval_results__train['r2']['std']))\n",
        "print('-- RMSE: {0:>8.2f} +/- {1:3.2f}'.format(eval_results__train['rmse']['mean'], eval_results__train['rmse']['std']))\n",
        "print('-- MAE:  {0:>8.2f} +/- {1:3.2f}'.format(eval_results__train['mae']['mean'], eval_results__train['mae']['std']))\n",
        "print('-- MAPE: {0:>8.1f} +/- {1:3.1f}%'.format(100*eval_results__train['mape']['mean'], 100*eval_results__train['mape']['std']))\n",
        "print('-- RAcc relative: {0:>8.2f} +/- {1:3.2f}%'.format(100*eval_results__train['racc']['mean'], 100*eval_results__train['racc']['std']))\n",
        "print('\\nModel Performance (Test)')\n",
        "print('# of Samples: {0:d}'.format(n_samples__test))\n",
        "print('-- R^2:  {0:>8.4f}'.format(eval_results__test['r2']))\n",
        "print('-- RMSE: {0:>8.2f}'.format(eval_results__test['rmse']))\n",
        "print('-- MAE:  {0:>8.2f}'.format(eval_results__test['mae']))\n",
        "print('-- MAPE: {0:>8.1f}%'.format(100*eval_results__test['mape']))\n",
        "print('-- RAcc relative : {0:>8.2f}%'.format(100*eval_results__test['racc']))\n",
        "print()\n",
        "\n",
        "# Heatmap: Predicted vs. measured values\n",
        "n_bins = 100\n",
        "bins = np.linspace(target_range[0], target_range[1], n_bins)\n",
        "title = 'DPhi Base Model - Heatmap\\nValidation Set ($R^2$ = {0:3.2f})'.format(r2)\n",
        "xlabel = 'Measured {0:s}\\n({1:s})'.format(target_label, target_units)\n",
        "ylabel = 'Predicted {0:s}\\n({1:s})'.format(target_label, target_units)\n",
        "output_filepath = '{0:s}/predicted_vs_measured__heatmap__validation_dataset.png'.format(output_path)\n",
        "plot_heatmap(y_meas, y_pred,\n",
        "             title=title, xlabel=xlabel, ylabel=ylabel,\n",
        "             x_bins=bins, y_bins=bins, plot_diagonal=True, \n",
        "             output_filepath=output_filepath)\n",
        "\n",
        "# Accuracy: Predicted vs. measured values\n",
        "title = 'DPhi Base Model - Accuracy\\nValidation Set ($R^2$ = {0:3.2f})'.format(r2)\n",
        "xlabel = 'Maximum Error\\n({0:s})'.format('%')\n",
        "ylabel = 'Percentage of Predictions\\n(%)'\n",
        "output_filepath = '{0:s}/predicted_vs_measured__accuracy__validation_dataset.png'.format(output_path)\n",
        "plot_regression_accuracy(y_meas, y_pred, max_error=25, error_type='relative', \n",
        "                         title=title, ylabel=ylabel, output_filepath=output_filepath)\n",
        "\n",
        "# Distributions of Predicted & Measured Values\n",
        "n_bins = 50\n",
        "bins = np.linspace(target_range[0], target_range[1], n_bins)\n",
        "data_labels = ['Measured', 'Predicted']\n",
        "title = 'DPhi Base Model - Distribution\\nValidation Set ($R^2$ = {0:3.2f})'.format(r2)\n",
        "xlabel = '{0:s} ({1:s})'.format(target_label, target_units)\n",
        "output_filepath = '{0:s}/predicted_vs_measured__distribution__validation_dataset.png'.format(output_path)\n",
        "plot_distribution([y_meas, y_pred], data_labels=data_labels, bins=bins, \n",
        "                  title=title, xlabel=xlabel, hist_units='probability', \n",
        "                  output_filepath=output_filepath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ef8423",
      "metadata": {
        "id": "87ef8423"
      },
      "source": [
        "## Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11162fef",
      "metadata": {
        "id": "11162fef"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbe1fbf",
      "metadata": {
        "id": "6dbe1fbf"
      },
      "outputs": [],
      "source": [
        "predictions = xgb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef0c5f7",
      "metadata": {
        "id": "aef0c5f7"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d4a4f9",
      "metadata": {
        "id": "29d4a4f9"
      },
      "source": [
        "## Export predictions into a .csv file for submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23e0541",
      "metadata": {
        "id": "d23e0541"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame({'prediction':predictions})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c240205",
      "metadata": {
        "id": "0c240205"
      },
      "outputs": [],
      "source": [
        "results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0e7fed",
      "metadata": {
        "id": "8a0e7fed"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv('predictions.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b68716e",
      "metadata": {
        "id": "1b68716e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}